{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe2537d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a3c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32cb14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wesad_subject(subject_path):\n",
    "    \n",
    "    with open(subject_path, 'rb') as file:\n",
    "        data = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    \n",
    "    chest_data = data['signal']['chest']\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['ACC_x'] = chest_data['ACC'][:, 0]\n",
    "    df['ACC_y'] = chest_data['ACC'][:, 1]\n",
    "    df['ACC_z'] = chest_data['ACC'][:, 2]\n",
    "    df['ECG']   = chest_data['ECG'].flatten()\n",
    "    df['EMG']   = chest_data['EMG'].flatten()\n",
    "    df['EDA']   = chest_data['EDA'].flatten()\n",
    "    df['Temp']  = chest_data['Temp'].flatten()\n",
    "    df['Resp']  = chest_data['Resp'].flatten()\n",
    "    \n",
    "    \n",
    "    df['label'] = data['label']\n",
    "    df['subject'] = data['subject']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93196c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df, window_seconds=2, stride_seconds=0.25, sampling_rate=700):\n",
    "    \n",
    "    window_steps = int(window_seconds * sampling_rate)\n",
    "    stride_steps = int(stride_seconds * sampling_rate)\n",
    "    \n",
    "    feature_cols = ['ACC_x', 'ACC_y', 'ACC_z', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "    data = df[feature_cols].values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(df) - window_steps, stride_steps):\n",
    "        window_data = data[i : i + window_steps]\n",
    "        window_labels = labels[i : i + window_steps]\n",
    "        \n",
    "        \n",
    "        mode_label = stats.mode(window_labels, keepdims=True)[0][0]\n",
    "        \n",
    "        \n",
    "        X_windows.append(window_data.transpose()) \n",
    "        y_windows.append(mode_label)\n",
    "        \n",
    "    return np.array(X_windows), np.array(y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95397f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading S2...\n",
      "Creating windows...\n",
      "Creating windows...\n",
      "Window shape: (7028, 8, 1400)\n",
      "Window shape: (7028, 8, 1400)\n",
      "Data Normalized. Mean: -0.00, Std: 1.00\n",
      "Data Normalized. Mean: -0.00, Std: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_path = r\"H:\\Research\\archive (1)\\WESAD\"\n",
    "\n",
    "subject_id = \"S2\"\n",
    "file_path = os.path.join(base_path, subject_id, f\"{subject_id}.pkl\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"Loading {subject_id}...\")\n",
    "    df = load_wesad_subject(file_path)\n",
    "\n",
    "    df = df[df['label'].isin([1, 2])].copy()\n",
    "    \n",
    "    label_map = {1: 0, 2: 1}\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "    \n",
    "    print(\"Creating windows...\")\n",
    "    X, y = create_windows(df, window_seconds=2, stride_seconds=0.25)\n",
    "    print(f\"Window shape: {X.shape}\") # Should be (N, 8, 1400)\n",
    "\n",
    "\n",
    "    N, C, T = X.shape\n",
    "    \n",
    "   \n",
    "    X_flat = X.transpose(0, 2, 1).reshape(-1, C)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "    X_norm = X_scaled_flat.reshape(N, T, C).transpose(0, 2, 1)\n",
    "    \n",
    "    print(f\"Data Normalized. Mean: {X_norm.mean():.2f}, Std: {X_norm.std():.2f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: File not found at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32d9a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old unique labels: [0 1 2 3 4 6 7]\n",
      "New unique labels (should be [0 1]): [0 1]\n",
      "Final dataset shape: (7036, 8, 1400)\n",
      "New unique labels (should be [0 1]): [0 1]\n",
      "Final dataset shape: (7036, 8, 1400)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Old unique labels:\", np.unique(y_s2))\n",
    "\n",
    "# 1. Identify indices where label is either 1 (Baseline) or 2 (Stress)\n",
    "# We ignore 0 (Transient), 3 (Amusement), 4 (Meditation) for this binary task\n",
    "valid_indices = np.where((y_s2 == 1) | (y_s2 == 2))[0]\n",
    "\n",
    "# 2. Keep only those valid windows\n",
    "X_final = X_s2[valid_indices]\n",
    "y_final = y_s2[valid_indices]\n",
    "\n",
    "# 3. Remap the labels to be 0 and 1\n",
    "# Original 1 becomes 0\n",
    "# Original 2 becomes 1\n",
    "y_final = np.where(y_final == 1, 0, 1)\n",
    "\n",
    "print(\"New unique labels (should be [0 1]):\", np.unique(y_final))\n",
    "print(f\"Final dataset shape: {X_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e47f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e5a2158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5622 | Test samples: 1406\n"
     ]
    }
   ],
   "source": [
    "# Convert to Tensors\n",
    "tensor_x = torch.Tensor(X_norm)\n",
    "tensor_y = torch.Tensor(y).long()\n",
    "\n",
    "# Time-Based Split (First 80% Train, Last 20% Test)\n",
    "split_idx = int(0.8 * len(tensor_x))\n",
    "\n",
    "X_train = tensor_x[:split_idx]\n",
    "y_train = tensor_y[:split_idx]\n",
    "X_test = tensor_x[split_idx:]\n",
    "y_test = tensor_y[split_idx:]\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(X_train)} | Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fd4ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders updated. You can now run the training loop.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Update Tensors with the FIXED data\n",
    "tensor_x = torch.Tensor(X_final) \n",
    "tensor_y = torch.Tensor(y_final).long() # .long() is required for labels\n",
    "\n",
    "# 2. Split again\n",
    "X_train, X_test, y_train, y_test = train_test_split(tensor_x, tensor_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Re-create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders updated. You can now run the training loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29ac4974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightweightModel(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(8, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=11200, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LightweightModel(nn.Module):\n",
    "    def __init__(self, input_channels=8, num_classes=2, window_size=1400):\n",
    "        super(LightweightModel, self).__init__()\n",
    "        \n",
    "        # Encoder (Client Side)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Classifier (Server Side)\n",
    "        final_dim = window_size // 4 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * final_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = LightweightModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6bec044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1 | Loss: 0.7344 | Acc: 64.39%\n",
      "Epoch 1 | Loss: 0.7344 | Acc: 64.39%\n",
      "Epoch 2 | Loss: 0.6564 | Acc: 64.64%\n",
      "Epoch 2 | Loss: 0.6564 | Acc: 64.64%\n",
      "Epoch 3 | Loss: 0.6513 | Acc: 64.64%\n",
      "Epoch 3 | Loss: 0.6513 | Acc: 64.64%\n",
      "Epoch 4 | Loss: 0.6501 | Acc: 64.64%\n",
      "Epoch 4 | Loss: 0.6501 | Acc: 64.64%\n",
      "Epoch 5 | Loss: 0.6498 | Acc: 64.64%\n",
      "Epoch 5 | Loss: 0.6498 | Acc: 64.64%\n",
      "Epoch 6 | Loss: 0.6497 | Acc: 64.64%\n",
      "Epoch 6 | Loss: 0.6497 | Acc: 64.64%\n",
      "Epoch 7 | Loss: 0.6497 | Acc: 64.64%\n",
      "Epoch 7 | Loss: 0.6497 | Acc: 64.64%\n",
      "Epoch 8 | Loss: 0.6498 | Acc: 64.64%\n",
      "Epoch 8 | Loss: 0.6498 | Acc: 64.64%\n",
      "Epoch 9 | Loss: 0.6497 | Acc: 64.64%\n",
      "Epoch 9 | Loss: 0.6497 | Acc: 64.64%\n",
      "Epoch 10 | Loss: 0.6497 | Acc: 64.64%\n",
      "Training Complete.\n",
      "Epoch 10 | Loss: 0.6497 | Acc: 64.64%\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} | Loss: {running_loss/len(train_loader):.4f} | Acc: {100*correct/total:.2f}%\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e539d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
