{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe2537d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "002a3c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "32cb14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wesad_subject(subject_path):\n",
    "    \"\"\"Loads a WESAD subject's .pkl file and extracts CHEST data.\"\"\"\n",
    "    with open(subject_path, 'rb') as file:\n",
    "        data = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # Extract Chest Data (700 Hz)\n",
    "    chest_data = data['signal']['chest']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    df['ACC_x'] = chest_data['ACC'][:, 0]\n",
    "    df['ACC_y'] = chest_data['ACC'][:, 1]\n",
    "    df['ACC_z'] = chest_data['ACC'][:, 2]\n",
    "    df['ECG']   = chest_data['ECG'].flatten()\n",
    "    df['EMG']   = chest_data['EMG'].flatten()\n",
    "    df['EDA']   = chest_data['EDA'].flatten()\n",
    "    df['Temp']  = chest_data['Temp'].flatten()\n",
    "    df['Resp']  = chest_data['Resp'].flatten()\n",
    "    \n",
    "    # Add Labels & Subject ID\n",
    "    df['label'] = data['label']\n",
    "    df['subject'] = data['subject']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_windows(df, window_seconds=2, stride_seconds=0.25, sampling_rate=700):\n",
    "    \"\"\"Slices the dataframe into overlapping windows.\"\"\"\n",
    "    window_steps = int(window_seconds * sampling_rate)\n",
    "    stride_steps = int(stride_seconds * sampling_rate)\n",
    "    \n",
    "    feature_cols = ['ACC_x', 'ACC_y', 'ACC_z', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "    data = df[feature_cols].values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    \n",
    "    for i in range(0, len(df) - window_steps, stride_steps):\n",
    "        window_data = data[i : i + window_steps]\n",
    "        window_labels = labels[i : i + window_steps]\n",
    "        \n",
    "        # Take the most frequent label in this window\n",
    "        mode_label = stats.mode(window_labels, keepdims=True)[0][0]\n",
    "        \n",
    "        X_windows.append(window_data.transpose()) \n",
    "        y_windows.append(mode_label)\n",
    "        \n",
    "    return np.array(X_windows), np.array(y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82e1bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading S2...\n",
      "Creating windows...\n",
      "Creating windows...\n",
      "Original Window shape: (7028, 8, 1400)\n",
      "Original Window shape: (7028, 8, 1400)\n",
      "Data Normalized. Mean: -0.00, Std: 1.00\n",
      "Ready for Training! Train samples: 5622 | Test samples: 1406\n",
      "Data Normalized. Mean: -0.00, Std: 1.00\n",
      "Ready for Training! Train samples: 5622 | Test samples: 1406\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "base_path = r\"H:\\Research\\archive (1)\\WESAD\"  # Your specific path\n",
    "subject_id = \"S2\"\n",
    "file_path = os.path.join(base_path, subject_id, f\"{subject_id}.pkl\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"Loading {subject_id}...\")\n",
    "    df = load_wesad_subject(file_path)\n",
    "\n",
    "    # 1. Filter Labels (1=Baseline, 2=Stress)\n",
    "    df = df[df['label'].isin([1, 2])].copy()\n",
    "    \n",
    "    # 2. Remap Labels to 0 (Baseline) and 1 (Stress)\n",
    "    label_map = {1: 0, 2: 1}\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "    \n",
    "    # 3. Create Windows\n",
    "    print(\"Creating windows...\")\n",
    "    X, y = create_windows(df, window_seconds=2, stride_seconds=0.25)\n",
    "    print(f\"Original Window shape: {X.shape}\") \n",
    "\n",
    "    # 4. Normalize (StandardScaler) - CRITICAL STEP\n",
    "    N, C, T = X.shape\n",
    "    X_flat = X.transpose(0, 2, 1).reshape(-1, C) # Flatten for scaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "    X_norm = X_scaled_flat.reshape(N, T, C).transpose(0, 2, 1) # Reshape back\n",
    "    \n",
    "    print(f\"Data Normalized. Mean: {X_norm.mean():.2f}, Std: {X_norm.std():.2f}\")\n",
    "\n",
    "    # 5. Time-Based Split (No Shuffling!)\n",
    "    # We use X_norm (the clean data), NOT X_final (the raw data)\n",
    "    tensor_x = torch.Tensor(X_norm)\n",
    "    tensor_y = torch.Tensor(y).long()\n",
    "\n",
    "    split_idx = int(0.8 * len(tensor_x))\n",
    "\n",
    "    X_train = tensor_x[:split_idx]\n",
    "    y_train = tensor_y[:split_idx]\n",
    "    X_test = tensor_x[split_idx:]\n",
    "    y_test = tensor_y[split_idx:]\n",
    "\n",
    "    # 6. Create DataLoaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"Ready for Training! Train samples: {len(X_train)} | Test samples: {len(X_test)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: File not found at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95397f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "class LightweightModel(nn.Module):\n",
    "    def __init__(self, input_channels=8, num_classes=2, window_size=1400):\n",
    "        super(LightweightModel, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        final_dim = window_size // 4 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * final_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = LightweightModel().to(device)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a00b1945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1 | Loss: 0.5667 | Acc: 97.39%\n",
      "Epoch 1 | Loss: 0.5667 | Acc: 97.39%\n",
      "Epoch 2 | Loss: 0.4009 | Acc: 93.33%\n",
      "Epoch 2 | Loss: 0.4009 | Acc: 93.33%\n",
      "Epoch 3 | Loss: 0.3735 | Acc: 92.28%\n",
      "Epoch 3 | Loss: 0.3735 | Acc: 92.28%\n",
      "Epoch 4 | Loss: 0.0606 | Acc: 98.70%\n",
      "Epoch 4 | Loss: 0.0606 | Acc: 98.70%\n",
      "Epoch 5 | Loss: 0.0483 | Acc: 99.52%\n",
      "Epoch 5 | Loss: 0.0483 | Acc: 99.52%\n",
      "Epoch 6 | Loss: 0.0396 | Acc: 99.57%\n",
      "Epoch 6 | Loss: 0.0396 | Acc: 99.57%\n",
      "Epoch 7 | Loss: 0.0296 | Acc: 99.54%\n",
      "Epoch 7 | Loss: 0.0296 | Acc: 99.54%\n",
      "Epoch 8 | Loss: 0.0713 | Acc: 98.42%\n",
      "Epoch 8 | Loss: 0.0713 | Acc: 98.42%\n",
      "Epoch 9 | Loss: 0.0898 | Acc: 99.50%\n",
      "Epoch 9 | Loss: 0.0898 | Acc: 99.50%\n",
      "Epoch 10 | Loss: 0.0300 | Acc: 99.57%\n",
      "Training Complete.\n",
      "Epoch 10 | Loss: 0.0300 | Acc: 99.57%\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} | Loss: {running_loss/len(train_loader):.4f} | Acc: {100*correct/total:.2f}%\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "24499bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Acc: 65.58%\n",
      "Attack (0.5): 63.02%\n",
      "Attack (0.5): 63.02%\n",
      "Attack (2.0): 12.73%\n",
      "Attack (2.0): 12.73%\n"
     ]
    }
   ],
   "source": [
    "def test_model_under_attack(model, loader, noise_level=0.0):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Attack: Add noise\n",
    "            noise = torch.randn_like(inputs) * noise_level\n",
    "            attacked_inputs = inputs + noise\n",
    "            \n",
    "            outputs = model(attacked_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return 100 * correct / total\n",
    "\n",
    "print(f\"Clean Acc: {test_model_under_attack(model, test_loader, 0.0):.2f}%\")\n",
    "print(f\"Attack (0.5): {test_model_under_attack(model, test_loader, 0.5):.2f}%\")\n",
    "print(f\"Attack (2.0): {test_model_under_attack(model, test_loader, 2.0):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b919ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
